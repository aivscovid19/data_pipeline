{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiteWorkerIntegratedV3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aivscovid19/data_pipeline/blob/gulnoza/SiteWorkerIntegratedV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIuKcSVRxOAr"
      },
      "source": [
        " ## **Set up and update**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQdKEHfAmSOv"
      },
      "source": [
        "!apt-get update && apt-get upgrade\n",
        "!pip install import_ipynb\n",
        "!pip install tld\n",
        "\n",
        "!apt autoremove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfL51qej0At1"
      },
      "source": [
        "#import modules\n",
        "!curl 'https://raw.githubusercontent.com/aivscovid19/data_pipeline/gulnoza/JobDispatcher.ipynb' > JobDispatcher.ipynb\n",
        "!curl 'https://raw.githubusercontent.com/aivscovid19/data_pipeline/gulnoza/miners/Rxiv_Miners.ipynb' > Rxiv_Miners.ipynb\n",
        "!curl 'https://raw.githubusercontent.com/aivscovid19/data_pipeline/gulnoza/miners/IbmcRuMiner.ipynb' > IbmcRuMiner.ipynb\n",
        "!curl 'https://raw.githubusercontent.com/aivscovid19/data_pipeline/gulnoza/miners/ScieloMiner.ipynb' > ScieloMinerV3.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptVLpn8gTfyW"
      },
      "source": [
        "# Define SiteWorkerIntegrated class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z06JvetHTBmQ"
      },
      "source": [
        "import import_ipynb\n",
        "import random, time\n",
        "import pandas as pd\n",
        "from pandas.io import gbq\n",
        "from tld import get_fld\n",
        "from JobDispatcher import JobDispatcher\n",
        "from IbmcRuMiner import IbmcRuMiner\n",
        "from Rxiv_Miners import ArxivMiner, MedrxivMiner, BiorxivMiner\n",
        "from ScieloMinerV3 import ScieloMiner"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP-sTZTfKsR8"
      },
      "source": [
        "class SiteWorkerIntegrated():\n",
        "  '''\n",
        "  SiteWorkerIntegrated class uses `site_worker_factory` method\n",
        "  to send data mining request to a domain-specific SiteWorker\n",
        "  given urls dataframe.\n",
        "\n",
        "  Attributes:\n",
        "    max_threshold (int): A default value of how many articles to upload\n",
        "                          to a BigQuery table at a time.\n",
        "    min_delay (int): A default value of min seconds to wait before the\n",
        "                      next request to a website is sent.\n",
        "    max_delay (int): A default value of max seconds to wait before the\n",
        "                      next request to a website is sent.\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    self.max_threshold = 50\n",
        "    self.min_delay = 0.1\n",
        "    self.max_delay = 2\n",
        "\n",
        "  def send_request(self, urls_df, limit):\n",
        "    ''' Finds domain from urls dataframe and sends request\n",
        "    to a domain-specific SiteWorker using site_worker_factory class method.\n",
        "\n",
        "    NOTE: An assumption is made that a given urls_dataframe contains\n",
        "          urls from a single domain, thus `send_request` method\n",
        "          checks only first entry in urls_dataframe 'article_url' field\n",
        "          to find a domain.\n",
        "    '''\n",
        "    # try:\n",
        "    url = urls_df.at[0, 'article_url'] \n",
        "    domain = get_fld(url)\n",
        "    print(domain)\n",
        "\n",
        "    site_worker = self.site_worker_factory(domain, urls_df, limit, self.driver_path)\n",
        "    site_worker.scrape_articles()\n",
        "  \n",
        "    # except KeyError:\n",
        "    #   print('The urls dataframe is empty')\n",
        "\n",
        "  def scrape_data(self, miner, urls_df, article_schema, limit=100):\n",
        "    ''' Scrapes data given urls' dataframe, and limit of articles to scrape.\n",
        "    Uploads data to a given BigQuery table and calls `update_job_status` method\n",
        "    from `JobDispatcher` class from `JobDispatcher` module\n",
        "    to update `status` of the job to `done`.\n",
        "\n",
        "    Attributes:\n",
        "      urls_df (pandas dataframe): A urls dataframe, to get a list of\n",
        "                                    article urls to scrape.\n",
        "      limit (int): A limit of articles to scrape. Default is 100.\n",
        "    '''\n",
        "\n",
        "    urls = [url for url in list(urls_df['article_url'])]\n",
        "    data = []\n",
        "    prev_count = 0\n",
        "    for count, url in enumerate(urls, 1):\n",
        "      miner.gather(url)\n",
        "      data.append(miner.results)\n",
        "      if (count == self.max_threshold or count == limit or count == len(urls)):\n",
        "        articles_list = list(filter(lambda i:\n",
        "                            i['title'] != None and len(i['title']) != 0 and\n",
        "                            i['abstract'] != None and len(i['abstract']) != 0,\n",
        "                            data[prev_count : count])) \n",
        "        articles_df = pd.DataFrame(articles_list)\n",
        "        articles_df.to_gbq(destination_table=f'{self.article_table}',\n",
        "                  project_id=self.project_id,\n",
        "                  if_exists='append',\n",
        "                  table_schema=article_schema,\n",
        "                  credentials=self.credentials)\n",
        "        JobDispatcher(self.credentials,\n",
        "                      self.project_id,\n",
        "                      self.url_table).update_job_status(urls_df.iloc[prev_count : count].copy())\n",
        "        prev_count = count\n",
        "      time.sleep(self.min_delay + self.max_delay * random.random())\n",
        "    return articles_df\n",
        "  \n",
        "  @classmethod\n",
        "  def init(cls, credentials, project_id, url_table, article_table, driver_path=None):\n",
        "    ''' Initializes `SiteWorkerIngrated` class\n",
        "\n",
        "    Attributes:\n",
        "      credentials (str): Credentials, either from user_account or service_account,\n",
        "                          to authenticate to Google Cloud APIs.\n",
        "      project_id (str): A project_id on Google Cloud Platform.\n",
        "      url_table (str): A url_table to use to retrieve urls_dataframe from,\n",
        "                        in form of `dataset_id.table_id`.\n",
        "      article_table (str): An article_table to use to upload scraped data to,\n",
        "                        in form of `dataset_id.table_id`.\n",
        "      driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    '''\n",
        "    cls.credentials = credentials\n",
        "    cls.project_id = project_id\n",
        "    cls.url_table = url_table\n",
        "    cls.article_table = article_table\n",
        "    cls.driver_path = driver_path\n",
        "\n",
        "  @classmethod\n",
        "  def site_worker_factory(cls, domain_name, urls_df, limit=100, driver_path=None):\n",
        "    ''' Sends a scraping request to a domain-specific SiteWorker\n",
        "\n",
        "    Attributes:\n",
        "      domain_name (str): A domain.\n",
        "      urls_df (pandas dataframe): A urls dataframe.\n",
        "      limit (int): A limit of articles to scrape. Default is 100.\n",
        "      driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    '''\n",
        "    site_worker = {'ibmc.msk.ru': IbmcRuSiteWorker(urls_df, limit, driver_path),\n",
        "                   'arxiv.org': ArxivSiteWorker(urls_df, limit, driver_path),\n",
        "                   'biorxiv.org': BiorxivSiteWorker(urls_df, limit, driver_path),\n",
        "                   'medrxiv.org': MedrxivSiteWorker(urls_df, limit, driver_path),\n",
        "                   'scielo.br': ScieloSiteWorker(urls_df, limit, driver_path)\n",
        "    }\n",
        "    return site_worker[domain_name]"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVmd-lAQTm03"
      },
      "source": [
        "## Define IbmcRuSiteWorker class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEN1V06rBaLy"
      },
      "source": [
        "class IbmcRuSiteWorker(SiteWorkerIntegrated):\n",
        "  '''\n",
        "  SiteWorker for http://pbmc.ibmc.msk.ru/\n",
        "\n",
        "  Attributes:\n",
        "    urls_df (pandas dataframe): A urls dataframe.\n",
        "    limit (int): A limit of articles to scrape. Default is 100.\n",
        "    driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    article_schema (list of dicts): An default article_schema for\n",
        "                                      a given SiteWorker.\n",
        "  '''\n",
        "  def __init__(self, urls_df, limit=100, driver_path=None):\n",
        "    super().__init__()\n",
        "    self.urls_df = urls_df\n",
        "    self.limit = limit\n",
        "    self.driver_path = driver_path\n",
        "    self.article_schema = [\n",
        "      {'name': 'abstract',                'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "      {'name': 'authors',                 'type': 'STRING'                    },\n",
        "      {'name': 'date_publication',        'type': 'DATE'                      },\n",
        "      {'name': 'doi',                     'type': 'STRING'                    },\n",
        "      {'name': 'extra_link',              'type': 'STRING'                    },\n",
        "      {'name': 'keywords',                'type': 'STRING'                    },\n",
        "      {'name': 'license',                 'type': 'STRING'                    },\n",
        "      {'name': 'organization_affiliated', 'type': 'STRING'                    },\n",
        "      {'name': 'pubmed_link',             'type': 'STRING'                    },\n",
        "      {'name': 'source',                  'type': 'STRING'                    },\n",
        "      {'name': 'title',                   'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "      {'name': 'translated_link',         'type': 'STRING',                   },\n",
        "      {'name': 'url',                     'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "      {'name': 'date_aquisition',         'type': 'DATE'                      },           \n",
        "    ]\n",
        "\n",
        "  def scrape_articles(self):\n",
        "    miner = IbmcRuMiner.IbmcEngine(IbmcRuMiner.IbmcLocations, driver_path=self.driver_path)\n",
        "    self.scrape_data(miner, self.urls_df, self.article_schema, self.limit)"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVQSsu3pTu5O"
      },
      "source": [
        "## Define ArxivSiteWorker class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNB8h54GTcrw"
      },
      "source": [
        "class ArxivSiteWorker(SiteWorkerIntegrated):\n",
        "  '''\n",
        "  SiteWorker for https://arxiv.org/\n",
        "\n",
        "  Attributes:\n",
        "    urls_df (pandas dataframe): A urls dataframe.\n",
        "    limit (int): A limit of articles to scrape. Default is 100.\n",
        "    driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    article_schema (list of dicts): An default article_schema for\n",
        "                                      a given SiteWorker.\n",
        "  '''\n",
        "  def __init__(self, urls_df, limit=100, driver_path=None):\n",
        "    super().__init__()\n",
        "    self.urls_df = urls_df\n",
        "    self.limit = limit\n",
        "    self.driver_path = driver_path\n",
        "    self.article_schema = [\n",
        "      {\"name\": \"abstract\",                \"type\": \"STRING\", \"mode\": \"REQUIRED\" },\n",
        "      {\"name\": \"authors\",                 \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"body\",                    \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"category\",                \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"date_aquisition\",         \"type\": \"DATE\"                       },\n",
        "      {\"name\": \"date_publication\",        \"type\": \"DATE\", \"mode\": \"REQUIRED\"   },\n",
        "      {\"name\": \"doi\",                     \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"extra_link\",              \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"keywords\",                \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"license\",                 \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"organization\",            \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"quantity_of_citations\",   \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"references\",              \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"search_keyword\",          \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"source\",                  \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"source_impact_factor\",    \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"title\",                   \"type\": \"STRING\", \"mode\": \"REQUIRED\" },\n",
        "      {\"name\": \"url\",                     \"type\": \"STRING\"                     }\n",
        "    ]\n",
        "\n",
        "  def scrape_articles(self):\n",
        "    miner = ArxivMiner.ArxivEngine(ArxivMiner.ArxivLocations, driver_path=self.driver_path)\n",
        "    self.scrape_data(miner, self.urls_df, self.article_schema, self.limit)\n"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZFP4gSST0YX"
      },
      "source": [
        "## Define BiorxivSiteWorker class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Z2uOfgTdYB"
      },
      "source": [
        "class BiorxivSiteWorker(SiteWorkerIntegrated):\n",
        "  '''\n",
        "  SiteWorker for https://www.biorxiv.org/\n",
        "\n",
        "  Attributes:\n",
        "    urls_df (pandas dataframe): A urls dataframe.\n",
        "    limit (int): A limit of articles to scrape. Default is 100.\n",
        "    driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    article_schema (list of dicts): An default article_schema for\n",
        "                                      a given SiteWorker.\n",
        "  '''\n",
        "  def __init__(self, urls_df, limit=100, driver_path=None):\n",
        "    super().__init__()\n",
        "    self.urls_df = urls_df\n",
        "    self.limit = limit\n",
        "    self.driver_path = driver_path\n",
        "    self.article_schema = [\n",
        "      {\"name\": \"abstract\",                \"type\": \"STRING\", \"mode\": \"REQUIRED\" },\n",
        "      {\"name\": \"authors\",                 \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"body\",                    \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"category\",                \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"date_aquisition\",         \"type\": \"DATE\"                       },\n",
        "      {\"name\": \"date_publication\",        \"type\": \"DATE\", \"mode\": \"REQUIRED\"   },\n",
        "      {\"name\": \"doi\",                     \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"extra_link\",              \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"keywords\",                \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"license\",                 \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"organization\",            \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"quantity_of_citations\",   \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"references\",              \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"search_keyword\",          \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"source\",                  \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"source_impact_factor\",    \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"title\",                   \"type\": \"STRING\", \"mode\": \"REQUIRED\" },\n",
        "      {\"name\": \"url\",                     \"type\": \"STRING\"                     }\n",
        "    ]\n",
        "\n",
        "  def scrape_articles(self):\n",
        "    miner = BiorxivMiner.BiorxivEngine(BiorxivMiner.BiorxivLocations, driver_path=self.driver_path)\n",
        "    self.scrape_data(miner, self.urls_df, self.article_schema, self.limit)\n"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg0_LFwaT0Z0"
      },
      "source": [
        "## Define MedrxivSiteWorker class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok996OLpTd5S"
      },
      "source": [
        "class MedrxivSiteWorker(SiteWorkerIntegrated):\n",
        "  '''\n",
        "  SiteWorker for https://www.medrxiv.org/\n",
        "\n",
        "  Attributes:\n",
        "    urls_df (pandas dataframe): A urls dataframe.\n",
        "    limit (int): A limit of articles to scrape. Default is 100.\n",
        "    driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    article_schema (list of dicts): An default article_schema for\n",
        "                                      a given SiteWorker.\n",
        "  '''\n",
        "  def __init__(self, urls_df, limit=100, driver_path=None):\n",
        "    super().__init__()\n",
        "    self.urls_df = urls_df\n",
        "    self.limit = limit\n",
        "    self.driver_path = driver_path\n",
        "    self.article_schema = [\n",
        "      {\"name\": \"abstract\",                \"type\": \"STRING\", \"mode\": \"REQUIRED\" },\n",
        "      {\"name\": \"authors\",                 \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"body\",                    \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"category\",                \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"date_publication\",        \"type\": \"DATE\", \"mode\": \"REQUIRED\"   },\n",
        "      {\"name\": \"doi\",                     \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"extra_link\",              \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"keywords\",                \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"license\",                 \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"organization\",            \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"quantity_of_citations\",   \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"references\",              \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"search_keyword\",          \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"source\",                  \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"source_impact_factor\",    \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"title\",                   \"type\": \"STRING\", \"mode\": \"REQUIRED\" },\n",
        "      {\"name\": \"url\",                     \"type\": \"STRING\"                     },\n",
        "      {\"name\": \"date_aquisition\",         \"type\": \"DATE\"                       }\n",
        "    ]\n",
        "\n",
        "  def scrape_articles(self):\n",
        "    miner = MedrxivMiner.MedrxivEngine(MedrxivMiner.MedrxivLocations, driver_path=self.driver_path)\n",
        "    self.scrape_data(miner, self.urls_df, self.article_schema, self.limit)"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBm7fhaiRaec"
      },
      "source": [
        "## Define ScieloSiteWorker class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05iVVMPgRkFo"
      },
      "source": [
        "class ScieloSiteWorker(SiteWorkerIntegrated):\n",
        "  '''\n",
        "  SiteWorker for https://www.scielo.org/\n",
        "\n",
        "  Attributes:\n",
        "    urls_df (pandas dataframe): A urls dataframe.\n",
        "    limit (int): A limit of articles to scrape. Default is 100.\n",
        "    driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    article_schema (list of dicts): An default article_schema for\n",
        "                                      a given SiteWorker.\n",
        "  '''\n",
        "  def __init__(self, urls_df, limit=100, driver_path=None):\n",
        "    super().__init__()\n",
        "    self.urls_df = urls_df\n",
        "    self.limit = limit\n",
        "    self.driver_path = driver_path\n",
        "    self.article_schema = [\n",
        "    {\"name\": \"abstract\",                \"type\": \"STRING\", \"mode\": \"REQUIRED\"  },\n",
        "    {\"name\": \"acquisition_date\",        \"type\": \"DATE\"                        },\n",
        "    {\"name\": \"authors\",                 \"type\": \"STRING\"                      },\n",
        "    {\"name\": \"body\",                    \"type\": \"STRING\"                      },\n",
        "    {\"name\": \"date\",                    \"type\": \"DATE\"                        },\n",
        "    {\"name\": \"doi\",                     \"type\": \"STRING\"                      },\n",
        "    {\"name\": \"keywords\",                \"type\": \"STRING\"                      },\n",
        "    {\"name\": \"link\",                    \"type\": \"STRING\"                      },\n",
        "    {\"name\": \"organization_affiliated\", \"type\": \"STRING\"                      },\n",
        "    {\"name\": \"pdf_link\",                \"type\": \"STRING\"                      },\n",
        "    {\"name\": \"references\",              \"type\": \"STRING\"                      },\n",
        "    {\"name\": \"source\",                  \"type\": \"STRING\"                      },\n",
        "    {\"name\": \"title\",                   \"type\": \"STRING\", \"mode\": \"REQUIRED\"  },\n",
        "    {\"name\": \"id\",                      \"type\": \"STRING\", \"mode\": \"REQUIRED\"  }\n",
        "  ]\n",
        "\n",
        "  def scrape_articles(self):\n",
        "    miner = ScieloMiner.ScieloEngine(ScieloMiner.ScieloLocations, driver_path=self.driver_path)\n",
        "    self.scrape_data(miner, self.urls_df, self.article_schema, self.limit)"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uVQdoPf3zeo"
      },
      "source": [
        "## Test SiteWorkers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7jqOenN3vGc"
      },
      "source": [
        "from google.colab import auth\n",
        "import uuid\n",
        "\n",
        "credentials = auth.authenticate_user()\n",
        "project_id = 'for-gulnoza'\n",
        "driver_path = '/usr/lib/chromium-browser/chromedriver'\n",
        "\n",
        "worker_id = uuid.uuid1()\n",
        "limit = 10\n"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS5W2qwOR_8K"
      },
      "source": [
        "\"\"\"\n",
        "# TEST SCIELO\n",
        "\n",
        "urls_table = 'scielo_urlbuilder.scielo_urls_v1'\n",
        "article_table = 'scielo_urlbuilder.articles_v2'\n",
        "\n",
        "urls_df = JobDispatcher(credentials, project_id, urls_table).register_job(worker_id, limit)\n",
        "\n",
        "articles = SiteWorkerIntegrated()\n",
        "articles.init(credentials, project_id, urls_table, article_table, driver_path=driver_path)\n",
        "articles.send_request(urls_df, limit)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rEQnGc38lbS"
      },
      "source": [
        "\"\"\"\n",
        "# TEST ARXIV\n",
        "\n",
        "urls_table = 'arxiv_test.url_builder_virus'\n",
        "article_table = 'arxiv_test.articles_v1'\n",
        "\n",
        "urls_df = JobDispatcher(credentials, project_id, urls_table).register_job(worker_id, limit)\n",
        "\n",
        "articles = SiteWorkerIntegrated()\n",
        "articles.init(credentials, project_id, urls_table, article_table, driver_path=driver_path)\n",
        "articles.send_request(urls_df, limit)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFYgXsDG8AIU"
      },
      "source": [
        "\"\"\"\n",
        "# TEST BIORXIV\n",
        "\n",
        "urls_table = 'biorxiv_test.url_builder_virus'\n",
        "article_table = 'biorxiv_test.articles_v1'\n",
        "\n",
        "urls_df = JobDispatcher(credentials, project_id, urls_table).register_job(worker_id, limit)\n",
        "\n",
        "articles = SiteWorkerIntegrated()\n",
        "articles.init(credentials, project_id, urls_table, article_table, driver_path=driver_path)\n",
        "articles.send_request(urls_df, limit)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WadRMdTE72ga"
      },
      "source": [
        "\"\"\"\n",
        "# TEST MEDRXIV\n",
        "\n",
        "urls_table = 'medrxiv_test.url_builder_virus'\n",
        "article_table = 'medrxiv_test.articles_v1'\n",
        "\n",
        "urls_df = JobDispatcher(credentials, project_id, urls_table).register_job(worker_id, limit)\n",
        "\n",
        "articles = SiteWorkerIntegrated()\n",
        "articles.init(credentials, project_id, urls_table, article_table, driver_path=driver_path)\n",
        "articles.send_request(urls_df, limit)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}