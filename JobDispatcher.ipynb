{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JobDispatcher.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPi2mlGLE8XtxljOyBcNbl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aivscovid19/data_pipeline/blob/gulnoza/JobDispatcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFnuWkAEa3er"
      },
      "source": [
        "## **Set up and update**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0iduNxOa0vL"
      },
      "source": [
        "from google.colab import auth\n",
        "#credentials = auth.authenticate_user()\n",
        "\n",
        "!apt-get update && apt-get upgrade\n",
        "!apt install chromium-chromedriver\n",
        "!pip install centaurMiner==0.0.8\n",
        "\n",
        "!apt autoremove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7XmL9FfatnV"
      },
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from pandas.io import gbq\n",
        "\n",
        "class JobDispatcher():\n",
        "  '''\n",
        "  JobDispatcher class retrieves urls_dataframe from a given BigQuery table,\n",
        "  updates `status` of a job and returns urls_dataframefor further processing.\n",
        "\n",
        "  Attributes:\n",
        "    credentials (str): Credentials, either from user_account or service_account,\n",
        "                        to authenticate to Google Cloud APIs.\n",
        "    project_id (str): A project_id on Google Cloud Platform.\n",
        "    url_table (str): A url_table to use to retrieve urls_dataframe from,\n",
        "                      in form of `dataset_id.table_id`.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, credentials, project_id, url_table):\n",
        "    self.credentials = credentials\n",
        "    self.project_id = project_id\n",
        "    self.url_table = url_table\n",
        "    self.url_schema = [\n",
        "        {'name': 'article_url', 'type': 'STRING',   'mode': 'REQUIRED'},\n",
        "        {'name': 'catalog_url', 'type': 'STRING',   'mode': 'REQUIRED'},\n",
        "        {'name': 'is_pdf',      'type': 'INTEGER',  'mode': 'REQUIRED'},\n",
        "        {'name': 'language',    'type': 'STRING',                     },\n",
        "        {'name': 'status',      'type': 'STRING',   'mode': 'REQUIRED'},\n",
        "        {'name': 'timestamp',   'type': 'DATETIME', 'mode': 'REQUIRED'},\n",
        "        {'name': 'worker_id',   'type': 'STRING',                     },\n",
        "        {'name': 'meta_info',   'type': 'STRING',                     },\n",
        "    ]\n",
        "\n",
        "  def register_job(self, worker_id, limit=100):\n",
        "    ''' Retrieves urls_dataframe from a given BigQuery table,\n",
        "    updates `worker_id`, and `status` of a job to `working on`,\n",
        "    and uploades updated dataframe to a given BigQuery table. \n",
        "\n",
        "    Attributes:\n",
        "      worker_id (str): A worker id responsible for this job.\n",
        "      limit (str): A limit of urls to retrieve from the given BigQuery table.\n",
        "                     Default is 100.\n",
        "\n",
        "    Returns:\n",
        "      urls_df(pandas dataframe): urls dataframe\n",
        "    '''\n",
        "\n",
        "    query = f\"\"\"\n",
        "      SELECT article_url, catalog_url, is_pdf, language, meta_info \n",
        "      FROM (SELECT *, ROW_NUMBER() OVER\n",
        "            (PARTITION BY article_url\n",
        "            ORDER BY timestamp DESC) AS rank\n",
        "            FROM {self.url_table})\n",
        "      WHERE rank = 1 AND status = \"not mined\" AND is_pdf = 0\n",
        "      ORDER BY timestamp\n",
        "      LIMIT {limit}\n",
        "    \"\"\"\n",
        "    urls_df = pd.read_gbq(query=query, project_id=self.project_id,\n",
        "                      credentials=self.credentials)\n",
        "    \n",
        "    # Update status on URLBuilder table and upload it to BQ\n",
        "    urls_df['status'] = 'working on'\n",
        "    urls_df['timestamp'] = datetime.utcnow()\n",
        "    urls_df['worker_id'] = worker_id\n",
        "    urls_df.to_gbq(destination_table=self.url_table,\n",
        "              project_id=self.project_id,\n",
        "              if_exists='append',\n",
        "              table_schema=self.url_schema,\n",
        "              credentials=self.credentials)\n",
        "    return urls_df\n",
        "  \n",
        "  def update_job_status(self, urls_df):\n",
        "    ''' Updates `status' of the job to `done`\n",
        "    and uploades updated dataframe to a given BigQuery table.\n",
        "\n",
        "    Attributes:\n",
        "      urls_df (pandas dataframe): A urls dataframe that was retrieved using\n",
        "      `register_job` method. \n",
        "    '''\n",
        "\n",
        "    urls_df['status'] = 'done'\n",
        "    urls_df['timestamp'] = datetime.utcnow()\n",
        "    urls_df.to_gbq(destination_table=self.url_table,\n",
        "              project_id=self.project_id,\n",
        "              if_exists='append',\n",
        "              table_schema=self.url_schema,\n",
        "              credentials=self.credentials)\n",
        "    print(\"Done\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}