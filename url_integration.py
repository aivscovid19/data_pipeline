# -*- coding: utf-8 -*-
"""URL_integration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J17Kp-vGlOxUBAPkvMVKZd9rTl7rGVMq
"""

# Installing Centaurminer chrome driver
!pip install centaurMiner==0.1.0
# Colab only: This is normally done automatically
!apt-get update # update ubuntu to correctly run apt-install
!apt install chromium-chromedriver # Installs to '/usr/lib/chromium-browser/chromedriver'
import time
import pandas_gbq
import pandas as pd
from datetime import datetime

from google.colab import auth
credentials = auth.authenticate_user()

import centaurminer as mining

class URL_builder():
  """
    Base class for building URLs for different journals.

    ...

    Attributes
    ----------
    search_word : str
        word to be searched

    Methods
    -------
    url_collector():
        Gets URLs from pages.
    create_url_schema(list_of_urls,catalog_url,is_pdf,lang):
        Create url schema to push into bigquery.
    send_to_bigquery(project_id,table_id):
        To push url_schema to bigquery.
    
  """
  
  def __init__(self,search_word):
    """
        Constructs all the necessary attributes for the URL_builder object.

        Parameters
        ----------
            search_word : str
              word to be searched
        """
    self._url_list=[]
    self.total_urls=[]
    self.valid_urls=[]
    self._search_word=search_word
    self._driver_path = '/usr/lib/chromium-browser/chromedriver'
    self._anchor_element={'arxiv':'span.list-identifier > a','biorxiv':'a.highwire-cite-linked-title','medrxiv':'a.highwire-cite-linked-title','preprint':'a#title.title',
            'pbmc':'a.class1','jamanetwork':'h3.article--title > a'}
    self._url_base={'arxiv':f"http://export.arxiv.org/find/all/1/all:+{search_word}/0/1/0/all/0/1?skip=".format(search_word),
              'biorxiv':f'https://www.biorxiv.org/search/{search_word}%20numresults%3A75%20sort%3Apublication-date%20direction%3Adescending?page='.format(search_word),
              'medrxiv':f'https://www.medrxiv.org/search/{search_word}%20numresults%3A75%20sort%3Apublication-date%20direction%3Adescending?page='.format(search_word),
              'preprint':f"https://www.preprints.org/search?search1={search_word}&field1=article_abstract&field2=authors&clause=AND&search2=&page_num=".format(search_word),
              'pbmc':f'http://pbmc.ibmc.msk.ru/ru/search-ru/?search={search_word}&fn=5'.format(search_word),
              'jamanetwork':f'https://jamanetwork.com/searchresults?q={search_word}&sort=Newest&page='.format(search_word)
              }


    self._schema = [
            {'name': 'article_url', 'type': 'STRING',   'mode': 'REQUIRED'},
            {'name': 'catalog_url', 'type': 'STRING',   'mode': 'REQUIRED'},
            {'name': 'is_pdf',      'type': 'INTEGER',  'mode': 'REQUIRED'},
            {'name': 'language',    'type': 'STRING',                     },
            {'name': 'status',      'type': 'STRING',   'mode': 'REQUIRED'},
            {'name': 'timestamp',   'type': 'DATETIME', 'mode': 'REQUIRED'},
            {'name': 'worker_id',   'type': 'STRING',                     },
            {'name': 'meta_info',   'type': 'STRING',                     },
    ]
  
  

  @classmethod
  def urlbuilderfactory(cls,journal_name,search_word,limit):
    journaldictionary={'arxiv':arxiv_url(search_word,limit),
                       'preprint':preprint_url(search_word,limit),
                       'biorxiv':biorxiv_url(search_word,limit),
                       'medrxiv':medrxiv_url(search_word,limit),
                       'pbmc':pbmc_url(search_word,limit),
                       'scielo':scielo_url(search_word)
                       #'jamanetwork': jamanetwork_url(search_word, pages)
                       }
    return journaldictionary[journal_name]

  


  def _url_collector(self):
    '''
      Method to extract the article URLs
    '''
    if self.page_num is not None:
      self.page_url=self._url_base[self.journal]+str(self.page_num)
    else:
      self.page_url=self._url_base[self.journal]
    articles = mining.Element("css_selector",self._anchor_element[self.journal]).get_attribute('href')
    self.urls = mining.CollectURLs(self.page_url, articles, driver_path=self._driver_path)  
  

  
  
  def _create_url_schema(self,list_of_urls,catalog_url,is_pdf=None,lang='en'):
    '''
      Method to create a dataframe.

      Args:
      url1: URLs extracted.
      url: catalog URL
      is_pdf: Function to determine if the URL is PDF or not.
      lang: language of the data.

    Returns:
      Dataframe
    '''
    self._url_schema=pd.DataFrame({'article_url': list_of_urls})
    self._url_schema['catalog_url']=catalog_url
    if is_pdf==None:
      self._url_schema['is_pdf'] =0
    else:
      self._url_schema['is_pdf'] = self._url_schema['article_url'].apply(is_pdf)
    self._url_schema['language']=lang
    self._url_schema['status']='not mined'
    self._url_schema['timestamp']= datetime.utcnow()
    self._url_schema['worker_id']=None
    self._url_schema['meta_info']=None
    return self._url_schema
  
  
 

    
  def _send_to_bigquery(self,project_id,table_id):
    '''
        Method to push urls to bigquery.
        Args:
          project_id: project_id of bigquery
          table_id: table_id of project
    '''
    pandas_gbq.to_gbq(self._url_schema, table_id, project_id=project_id, if_exists='append', table_schema=self._schema)
    pass
  


  def get_urls(self,project_id,table_id):
    '''
      To be implemented by the corresponding subclasses
    '''
    raise NotImplementedError("Subclasses should implement this!")
  
  






class arxiv_url(URL_builder):
  """
        Child class for getting URLs from arxiv journal.

        Parameters
        ----------
            search_word : str
              word to be searched
        Parameters
        ----------
        search_word : str
              word to be searched
        limit: int
              number of URLs to be collected
  """

  def __init__(self,search_word,limit):
        super().__init__(search_word)  
        self.limit=limit
  

  def get_urls(self,project_id,table_id):
    '''
      Method to get URLs and send url_schema to bigquery.

      Args:
      project_id: project_id of bigquery
      table_id: table_id of project

    Returns:
      url_schema
    '''
    for self.page_num in range(0,25*(99999),25):
      self.journal='arxiv'
      print(f"\n Scraping from page {self.page_num}...", flush=True)
      is_pdf = lambda x: pd.Series({'is_pdf': 1 if x.find('pdf') != -1 else 0}) 
      self._url_collector()

      [self.valid_urls.append(i) for i in self.urls if 'format' not in i]   
      nums=min(self.limit - len(self.total_urls), len(self.valid_urls))
      self.total_urls.extend(self.valid_urls[:nums])

      self._url_list=[]
      [self._url_list.append(i) for i in self.valid_urls[:nums]]
      self._create_url_schema(self._url_list,self.page_url,is_pdf)
      self._send_to_bigquery(project_id,table_id)

      if len(self.total_urls) == self.limit:
        break
    print('\n Total no. of URLS:'+str(len(self._url_list)))
    return self._url_schema




class preprint_url(URL_builder):
  """
        Child class for getting URLs from preprint journal.

        Parameters
        ----------
            search_word : str
              word to be searched
        Parameters
        ----------
        search_word : str
              word to be searched
        limit: int
              number of URLs to be collected
  """

  def __init__(self,search_word,limit):
      super().__init__(search_word)  
      self.limit=limit
  
  def get_urls(self,project_id,table_id):
    '''
      Method to get URLs and send url_schema to bigquery.

      Args:
      project_id: project_id of bigquery
      table_id: table_id of project

    Returns:
      url_schema
    '''
    self.journal='preprint'
    for self.page_num in range(1,99999):
      print(f"\n Scraping from page {self.page_num}...", flush=True)
      self._url_collector()

      [self.valid_urls.append(i) for i in self.urls]  
      nums=min(self.limit - len(self.total_urls), len(self.valid_urls))
      self.total_urls.extend(self.valid_urls[:nums])

      self._url_list=[]
      [self._url_list.append(i) for i in self.valid_urls[:nums]]
      self._create_url_schema(self._url_list,self.page_url)
      self._send_to_bigquery(project_id,table_id)
      if len(self.total_urls) == self.limit:
        break
      
    print('\n Total no. of URLS:'+str(len(self._url_list)))
    return self._url_schema
    



class biorxiv_url(URL_builder):
  """
        Child class for getting URLs from biorxiv journal.

        Parameters
        ----------
            search_word : str
              word to be searched
        Parameters
        ----------
        search_word : str
              word to be searched
        limit: int
              number of URLs to be collected
  """

  def __init__(self,search_word,limit):
      super().__init__(search_word)  
      self.limit=limit
      self.journal='biorxiv'
  
  def get_urls(self,project_id,table_id):
    '''
      Method to get URLs and send url_schema to bigquery.

      Args:
      project_id: project_id of bigquery
      table_id: table_id of project

    Returns:
      url_schema
    '''
    for self.page_num in range(0,99999):
      print(f"\n Scraping from page {self.page_num}...", flush=True)
      self._url_collector()

      [self.valid_urls.append(i) for i in self.urls]  
      nums=min(self.limit - len(self.total_urls), len(self.valid_urls))
      self.total_urls.extend(self.valid_urls[:nums])

      self._url_list=[]
      [self._url_list.append(i) for i in self.valid_urls[:nums]]
      self._create_url_schema(self._url_list,self.page_url)
      self._send_to_bigquery(project_id,table_id)
      if len(self.total_urls) == self.limit:
        break
    print('\n Total no. of URLS:'+str(len(self._url_list)))
    return self._url_schema




class medrxiv_url(URL_builder):
  """
        Child class for getting URLs from medrxiv journal.

        Parameters
        ----------
            search_word : str
              word to be searched
        Parameters
        ----------
        search_word : str
              word to be searched
        limit: int
              number of URLs to be collected
  """

  def __init__(self,search_word,limit):
      super().__init__(search_word)  
      self.limit=limit
      self.journal='medrxiv'
  
  def get_urls(self,project_id,table_id):
    '''
      Method to get URLs and send url_schema to bigquery.

      Args:
      project_id: project_id of bigquery
      table_id: table_id of project

    Returns:
      url_schema
    '''
    for self.page_num in range(0,99999):
      print(f"\n Scraping from page {self.page_num}...", flush=True)
      self._url_collector()

      [self.valid_urls.append(i) for i in self.urls]  
      nums=min(self.limit - len(self.total_urls), len(self.valid_urls))
      self.total_urls.extend(self.valid_urls[:nums])

      self._url_list=[]
      [self._url_list.append(i) for i in self.valid_urls[:nums]]
      self._create_url_schema(self._url_list,self.page_url)
      self._send_to_bigquery(project_id,table_id)
      if len(self.total_urls) == self.limit:
        break
    print('\n Total no. of URLS:'+str(len(self._url_list)))
    return self._url_schema
  
  


class pbmc_url(URL_builder):
  """
        Child class for getting URLs from pbmc journal.

        Parameters
        ----------
            search_word : str
              word to be searched
        Parameters
        ----------
        search_word : str
              word to be searched
        limit: int
              number of URLs to be collected
  """

  def __init__(self,search_word,limit):
    super().__init__(search_word)
    self.limit=limit 
  
  def get_urls(self,project_id,table_id):
    '''
      Method to get URLs and send url_schema to bigquery.

      Args:
      project_id: project_id of bigquery
      table_id: table_id of project

    Returns:
      url_schema
    '''
    self.page_num=None
    self.journal='pbmc'
    print(f"\n Scraping from page...", flush=True)
    self._url_collector()
    [self.valid_urls.append(i) for i in self.urls if 'article-ru' in i]  
    nums=min(self.limit - len(self.total_urls), len(self.valid_urls))
    self.total_urls.extend(self.valid_urls[:nums])
    [self._url_list.append(i) for i in self.valid_urls[:nums]]
    self._create_url_schema(self._url_list,self.page_url,lang='ru')
    self._send_to_bigquery(project_id,table_id)
    print('\n Total no. of URLS:'+str(len(self._url_list)))
    return self._url_schema

url=URL_builder.urlbuilderfactory('preprint','virus',120)
url.get_urls('for-yr','Medical_Dataset.all_url')

url=URL_builder.urlbuilderfactory('arxiv','virus',80)
url.get_urls('for-yr','Medical_Dataset.all_url')

url=URL_builder.urlbuilderfactory('pbmc','virus',20)
url.get_urls('for-yr','Medical_Dataset.all_urls')

url=URL_builder.urlbuilderfactory('medrxiv','virus',60)
url.get_urls('for-yr','Medical_Dataset.all_urls')

url=URL_builder.urlbuilderfactory('biorxiv','virus',4)
url.get_urls('for-yr','Medical_Dataset.all_urls')